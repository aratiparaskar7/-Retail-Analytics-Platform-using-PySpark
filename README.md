# ğŸ“Š Retail Sales Analytics Pipeline using PySpark

An end-to-end **data engineering & analytics pipeline** built using **PySpark**, covering:

âœ” Data Cleaning  
âœ” ETL Pipeline  
âœ” Star Schema Data Modeling  
âœ” SQL-Based Analytics  
âœ” BI-Ready Output  

This project demonstrates real-world **data engineering workflows** using Apache Spark, including data preprocessing, transformation, modeling, and analytical querying.

---

## ğŸš€ Project Overview

This project processes raw retail transactional data and transforms it into a **cleaned, analytics-ready data warehouse** using a **Star Schema**.  
After building fact & dimension tables, SQL queries are executed to answer key business questions.

---

## ğŸ§± Features

### âœ… 1. Data Cleaning (PySpark)
- Handle missing values  
- Standardize column names  
- Convert data types  
- Remove duplicates  
- Normalize text  
- Currency formatting  
- Date cleaning & parsing

### âœ… 2. ETL Pipeline (Extract â†’ Transform â†’ Load)
- Load raw CSV datasets  
- Build `dim_users`, `dim_products`, `dim_date`  
- Create `fact_orders` using surrogate keys  
- Implement Spark-based joins  
- Save processed tables  

### âœ… 3. Data Modeling (Star Schema)

          dim_users
              |
dim_products â€” fact_orders â€” dim_date


### âœ… 4. SQL Analytics
Using Spark SQL to answer business questions such as:
- Monthly revenue trends  
- Best-selling products  
- High-value customers  
- Category-level analysis  
- Order dynamics  

---

## ğŸ“ Project Structure

```plaintext
Retail-Sales-Analytics-Pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_cleaning.ipynb
â”‚   â”œâ”€â”€ 02_etl_star_schema.ipynb
â”‚   â””â”€â”€ 03_sql_analytics.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â”œâ”€â”€ etl_star_schema.py
â”‚   â””â”€â”€ sql_analytics.py
â”‚
â”œâ”€â”€ README.md
```


---

## ğŸ› ï¸ Tech Stack

| Tool / Technology | Purpose |
|------------------|---------|
| **PySpark** | Distributed data processing |
| **Spark SQL** | Business analytics |
| **DataFrames** | Core ETL operations |
| **Google Colab / Jupyter** | Notebook environment |
| **Python 3.x** | Main language |

---

## ğŸ“Œ Datasets Used

- `users_cleaned.csv`  
- `products_cleaned.csv`  
- `orders_cleaned.csv`  
- `order_items_cleaned.csv`  
- `dim_date` (generated using Spark)

---

## ğŸ§¹ Data Cleaning Summary

Key cleaning tasks include:

- Dropping rows with missing critical fields  
- Standardizing text formatting  
- Normalizing column names (`lowercase + underscores`)  
- Removing unwanted characters using `regexp_replace`  
- Converting string dates into proper date types  
- Removing duplicate rows  
- Ensuring schema consistency  

---

## ğŸ—ï¸ ETL & Star Schema Steps

### **1. Build Dimension Tables**
- `dim_users` with surrogate key  
- `dim_products` with surrogate key  
- `dim_date` (year, month, day, weekday, quarter)

### **2. Build Fact Table**
- Merge orders + order_items  
- Add surrogate keys (`user_sk`, `product_sk`, `date_sk`)  
- Calculate sales metrics  

### **3. Save All Tables**
All dimension and fact tables stored under:

/data/processed/

---

## ğŸ“ˆ Sample SQL Analytics

### ğŸ”¹ Top 10 Products by Revenue
```sql
SELECT product_id, SUM(sales_amount) AS total_revenue
FROM fact_orders
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;

ğŸ”¹ Monthly Sales Trend
SELECT year, month, SUM(sales_amount) AS monthly_sales
FROM fact_orders
GROUP BY year, month
ORDER BY year, month;

ğŸ”¹ Most Valuable Customers
SELECT user_id, SUM(sales_amount) AS total_spent
FROM fact_orders
GROUP BY user_id
ORDER BY total_spent DESC;

ğŸ“¦ How to Run This Project
1. Install Dependencies
pip install pyspark

2. Run the ETL Scripts
python src/data_cleaning.py
python src/etl_star_schema.py
python src/sql_analytics.py

3. Or simply run the Colab/Jupyter Notebooks
Recommended for easier visualization.

â­ Key Learnings

From this project, you will learn:
How to design a real-world data pipeline
How to clean large datasets using PySpark
How to build a star schema (fact + dimension tables)
How to create surrogate keys in Spark
How to perform SQL analytics on Spark DataFrames
How to document and structure a Data Engineering project

Conclusion

This project showcases a complete Retail Analytics Pipeline using PySpark, transforming raw data into a fully modeled and analytics-ready warehouse.
It is ideal for:

Data Engineering portfolios
Spark learning
ETL + SQL case studies
GitHub showcase projects


